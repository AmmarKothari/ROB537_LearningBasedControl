\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Bandit Problem}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem Description}{1}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Results}{1}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Analysis}{1}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Grid World}{1}{section.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results for different tests for Bandit Problem with an Action Value learner}}{1}{table.1}}
\newlabel{tab:Bandit}{{1}{1}{Results for different tests for Bandit Problem with an Action Value learner}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of Action Value tables between selection for 10 steps and 100 steps for an action-value learner with 20\% epsilon greedy exploration on a multiarmed bandit problem.}}{2}{figure.1}}
\newlabel{fig:Bandit20}{{1}{2}{Comparison of Action Value tables between selection for 10 steps and 100 steps for an action-value learner with 20\% epsilon greedy exploration on a multiarmed bandit problem}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of Action Value tables between selection for 10 steps and 100 steps for an action-value learner with 0\% epsilon greedy exploration on a multiarmed bandit problem.}}{2}{figure.2}}
\newlabel{fig:Bandit0}{{2}{2}{Comparison of Action Value tables between selection for 10 steps and 100 steps for an action-value learner with 0\% epsilon greedy exploration on a multiarmed bandit problem}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Description}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Results}{2}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Analysis}{2}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{2}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning progress of both agents on grid world. Q learning agent is able to improve over the course of training, where as the Action Value agent never improves significantly.}}{3}{figure.3}}
\newlabel{fig:GridLearn}{{3}{3}{Learning progress of both agents on grid world. Q learning agent is able to improve over the course of training, where as the Action Value agent never improves significantly}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Converged Q-Value learning agent trajectories. Darker colors indicate a square that is visited most often. The test episodes only last 10 steps. The agent learns the most direct route to the highest reward.}}{3}{figure.4}}
\newlabel{fig:GridQTraj}{{4}{3}{Converged Q-Value learning agent trajectories. Darker colors indicate a square that is visited most often. The test episodes only last 10 steps. The agent learns the most direct route to the highest reward}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Converged Action Value learning agent trajectories. Darker colors indicate a square that is visited most often. The test episodes only last 10 steps. The agent learns to go down and right regardless of location.}}{3}{figure.5}}
\newlabel{fig:GridAVTraj}{{5}{3}{Converged Action Value learning agent trajectories. Darker colors indicate a square that is visited most often. The test episodes only last 10 steps. The agent learns to go down and right regardless of location}{figure.5}{}}
