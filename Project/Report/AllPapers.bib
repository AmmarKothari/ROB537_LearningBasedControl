  @inproceedings{byravan2017se3,
  title={SE3-nets: Learning rigid body motion using deep neural networks},
  author={Byravan, Arunkumar and Fox, Dieter},
  booktitle={Robotics and Automation (ICRA), 2017 IEEE International Conference on},
  pages={173--180},
  year={2017},
  organization={IEEE}}
  
@article{Ng2000,
    archivePrefix = {arXiv},
    arxivId = {arXiv:1011.1669v3},
    author = {Ng, Andrew and Russell, Stuart},
    doi = {10.2460/ajvr.67.2.323},
    eprint = {arXiv:1011.1669v3},
    file = {:Users/Ammar/Documents/Mendeley/Documents/Ng, Russell - 2000 - Algorithms for inverse reinforcement learning.pdf:pdf},
    isbn = {1-55860-707-2},
    issn = {00029645},
    journal = {Proceedings of the Seventeenth International Conference on Machine Learning},
    pages = {663--670},
    pmid = {16454640},
    title = {{Algorithms for inverse reinforcement learning}},
    url = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
    volume = {0},
    year = {2000}}

@article{Christiano2017,
    arxivId = {1706.03741},
    author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
    eprint = {1706.03741},
    file = {:Users/Ammar/Documents/Mendeley/Documents/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:pdf},
    journal = {arXi},
    title = {{Deep reinforcement learning from human preferences}},
    url = {http://arxiv.org/abs/1706.03741},
    year = {2017}}

@inproceedings{levine2015learning,
  title={Learning contact-rich manipulation skills with guided policy search},
  author={Levine, Sergey and Wagener, Nolan and Abbeel, Pieter},
  booktitle={Robotics and Automation (ICRA), 2015 IEEE International Conference on},
  pages={156--163},
  year={2015},
  organization={IEEE}}
  
@inproceedings{akrour2012april,
  title={April: Active preference learning-based reinforcement learning},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`e}le},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={116--131},
  year={2012},
  organization={Springer}}

@inproceedings{akrour2014programming,
  title={Programming by feedback},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`e}le and Souplet, Jean-Christophe},
  booktitle={International Conference on Machine Learning},
  number={32},
  pages={1503--1511},
  year={2014},
  organization={JMLR. org}}
  
@article{gu2016deep,
  title={Deep reinforcement learning for robotic manipulation},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  journal={ArXiv e-prints},
  year={2016}}
  
  
  @inproceedings{finn2016guided,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={49--58},
  year={2016}}


@article{finn2016generalizing,
  title={Generalizing Skills with Semi-Supervised Reinforcement Learning},
  author={Finn, Chelsea and Yu, Tianhe and Fu, Justin and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1612.00429},
  year={2016}}
  
  @inproceedings{johns2016deep,
  title={Deep learning a grasp function for grasping under gripper pose uncertainty},
  author={Johns, Edward and Leutenegger, Stefan and Davison, Andrew J},
  booktitle={Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
  pages={4461--4468},
  year={2016},
  organization={IEEE}}
  
  @article{Finn2016,
abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
archivePrefix = {arXiv},
arxivId = {1603.00448},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
doi = {1603.00448v3},
eprint = {1603.00448},
file = {:Users/Ammar/Documents/Mendeley/Documents/1603.00448.pdf:pdf},
isbn = {9781510829008},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
url = {http://arxiv.org/abs/1603.00448},
volume = {48},
year = {2016}}


@article{Akrour2014,
abstract = {This paper advocates a new ML-based programming framework, called Programming by Feedback (PF), which involves a sequence of interactions between the active computer and the user. The latter only provides preference judgments on pairs of solutions supplied by the active computer. The active computer involves two components: the learning component estimates the user's utility function and accounts for the user's (possibly limited) competence; the optimization component explores the search space and returns the most appropriate candidate solution. A proof of principle of the approach is proposed, showing that PF requires a handful of interactions in order to solve some discrete and continuous benchmark problems.},
author = {Akrour, Riad and Schoenauer, Marc and Souplet, Jean-Christophe and Sebag, Michele},
file = {:home/kotharia/Documents/Mendeley/Papers/schoenauer14.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of the 31st International Conference on Machine Learning},
pages = {1503--1511},
title = {{Programming by Feedback}},
volume = {32},
year = {2014}}


@inproceedings{mericcli2010biped,
  title={Biped Walk Learning Through Playback and Corrective Demonstration.},
  author={Meri{\c{c}}li, {\c{C}}etin and Veloso, Manuela M},
  booktitle={AAAI},
  pages={1594--1599},
  year={2010}}

@article{DAgger,
  author    = {St{\'{e}}phane Ross and
               Geoffrey J. Gordon and
               J. Andrew Bagnell},
  title     = {No-Regret Reductions for Imitation Learning and Structured Prediction},
  journal   = {CoRR},
  volume    = {abs/1011.0686},
  year      = {2010},
  url       = {http://arxiv.org/abs/1011.0686},
  timestamp = {Wed, 07 Jun 2017 14:41:06 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1011-0686},
  bibsource = {dblp computer science bibliography, http://dblp.org}}

@inproceedings{kalakrishnan2013learning,
  title={Learning objective functions for manipulation},
  author={Kalakrishnan, Mrinal and Pastor, Peter and Righetti, Ludovic and Schaal, Stefan},
  booktitle={Robotics and Automation (ICRA), 2013 IEEE International Conference on},
  pages={1331--1336},
  year={2013},
  organization={IEEE}}

@inproceedings{stulp2011learning,
  title={Learning to grasp under uncertainty},
  author={Stulp, Freek and Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  booktitle={Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  pages={5703--5708},
  year={2011},
  organization={IEEE}}
  
  
  @article{Levine2013,
abstract = {Direct policy search can e ectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requir- ing numerous samples and often falling into poor local optima. We present a guided pol- icy search algorithm that uses trajectory op- timization to direct policy learning and avoid poor local optima. We show how di erential dynamic programming can be used to gener- ate suitable guiding samples, and describe a regularized importance sampled policy opti- mization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.05611v1},
author = {Levine, Sergey and Koltun, Vladlen},
doi = {10.1109/ICRA.2015.7138994},
eprint = {arXiv:1501.05611v1},
file = {:Users/Ammar/Documents/Mendeley/Documents/levine13.pdf:pdf},
isbn = {9781479969227},
journal = {Proceedings of the 30th International Conference on Machine Learning},
title = {{Guided Policy Search}},
volume = {28},
year = {2013}
}

@article{SchulmanLMJA15,
  author    = {John Schulman and
               Sergey Levine and
               Philipp Moritz and
               Michael I. Jordan and
               Pieter Abbeel},
  title     = {Trust Region Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1502.05477},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.05477},
  archivePrefix = {arXiv},
  eprint    = {1502.05477},
  timestamp = {Wed, 07 Jun 2017 14:42:34 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/SchulmanLMJA15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}



@article{malvezzi2015syngrasp,
  title={Syngrasp: A matlab toolbox for underactuated and compliant hands},
  author={Malvezzi, Monica and Gioioso, Guido and Salvietti, Gionata and Prattichizzo, Domenico},
  journal={IEEE Robotics \& Automation Magazine},
  volume={22},
  number={4},
  pages={52--68},
  year={2015},
  publisher={IEEE}}

@inproceedings{malvezzi2013syngrasp,
  title={Syngrasp: a matlab toolbox for grasp analysis of human and robotic hands},
  author={Malvezzi, Monica and Gioioso, Guido and Salvietti, Gionata and Prattichizzo, Domenico and Bicchi, Antonio},
  booktitle={Robotics and Automation (ICRA), 2013 IEEE International Conference on},
  pages={1088--1093},
  year={2013},
  organization={IEEE}}